- [Model selection](#model-selection)
  - [Model selection](#model-selection-1)
    - [An estimator](#an-estimator)
    - [Error functions](#error-functions)
    - [Error vs loss function](#error-vs-loss-function)
    - [Hold-out method](#hold-out-method)
    - [Hold-out method: remarks](#hold-out-method-remarks)
  - [Model evaluation](#model-evaluation)
# Model selection
## Model selection
### An estimator
* $\hat{\theta} =\delta(ğ‘‡)$ (some function of your training data) â€“ an estimator
* Optimal parameter values?$\rightarrow$there can be many ways to compute them (MLE, shrinkageâ€¦)
  * There is no easy way to compare estimators in frequentist tradition
> Example: Linear regression
> * Estimator 1: $\theta=(ğ‘‹^ğ‘‡ ğ‘‹)^{-1} X^T ğ‘Œ$ (maximum likelihood)
> * Estimator 2: ğœ½=(0,â€¦,0,1)
> * Which one is better?
>   * A comparison strategy is needed!

* ML can lead to overfitting
* How can we find appropriate parameter values?
* How can we compare between different models? 
### Error functions
* Loss functions ğ¿(ğ‘¦,ğ‘¦Â Ì‚) used to evaluate the quality of training
* Error functions $ğ¸(ğ‘¦, \hat{ğ‘¦})$ used to measure the quality of prediction
  * Misclassification error
  $$ğ¸(y,\hat{ğ‘¦} )=
  \begin{cases}
    0, \quad y=\hat{y}\\
    1, \quad y\neq \hat{y}
  \end{cases}
  $$
  * Squared error $ğ¸(ğ‘¦, \hat{ğ‘¦} )=(ğ‘¦âˆ’\hat{ğ‘¦} )^2$
  * Formulas can be same, different purpose
### Error vs loss function
* Should error function be same as loss function?
* Normal practice: Choose the loss related to minus loglikelihood
> * Example: Predicting the risk of cancer:  
>  $$E(y=H,\hat{y}=C)=1\\
>  E(y=C,\hat{y}=H)=1000\\
>  loss matrix = (0 1000 1 0)
>  $$
* One can show: ${{ğ‘(ğ‘¦=ğ»â”‚ğ‘¥)}\over{ğ‘(ğ‘¦=ğ¶|ğ‘¥)}}>1000â†’ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘¦ ğ‘ğ‘  ğ»$

* Given a model, choose the optimal parameter values
  * Decision theory
* If we know the true distribution $ğ‘(ğ‘¦,ğ’™)$ then we choose the optimal model by minimizing the expected risk:
$$
\min_{\hat{y}}\bar{E}_{new}=\min_\theta \int_{(x_*,y_*)}E(y_*,\hat{y}(x_*,T,\theta))p(x_*,y_*)dx_*dy_*dT
$$
* Problem: data generating process is unknown$\rightarrow$can not compute expected risk!
* Approximation 1: Instead of considering all possible ğ‘‡, take only one ğ‘‡$\rightarrow$expected new data error
$$
E_{new}=\int_{(x_*,y_*)}E(y_*,\hat{y}(x_*,T,\theta))p(x_*,y_*)dx_*dy_*
$$
* Fix ğ‘‡ as a particular training set
* Approximation 2:  
  $$
  \int_{(x_*,y_*)}E(y_*,\hat{y}(x_*,T,\theta))p(x_*,y_*)dx_*dy_*
  \approx {1\over{|V|}}\sum_{(x_*,y_*)\in V}E(y_*,\hat{y}(x_*,T,\theta))
  $$



### Hold-out method
* Divide into training, validation and test sets
* Choose proportions in some way
* Given: training, validation, test sets  and models to select between
* Training set is to used for fitting models to the dataset by using given loss function
* Validation set is used to choose the best model (lowest risk)
* Test set is used to test a performance on a new data
### Hold-out method: remarks
* Data needs to be shuffled before split
* Method is suitable for large data otherwise training affected
* Proportions: increasing % of training data generally leads to better performance but the quality of â€Approximation 2â€ decreases



## Model evaluation
